{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"../resources/cropped-SummerWorkshop_Header.png\">  \n",
    "\n",
    "<h1 align=\"center\">Visual Behavior Ophys</h1> \n",
    "<h2 align=\"center\">Summer Workshop on the Dynamic Brain </h2> \n",
    "<h3 align=\"center\">August, 2021</h3> \n",
    "\n",
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>Here we build on the classification tutorial to apply these same concepts to real neural data\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply classification (decoding) to neural data\n",
    "The previous classification tutorial applied classification on low-dimensional (2D or 3D) datasets. We will now extend this concept to actual (higher dimensional) neural data from the Visual Behavior dataset. \n",
    "\n",
    "In the example below, we will attempt to classify the neural response based on the stimulus that elicited that response. This is a form of 'decoding'. In other words, we will attempt to decode the stimulus based only on the neural response.\n",
    "\n",
    "In the following example, the number of feature dimensions will be much higher. We will use the average response of each simultaneously recorded neuron in a short window following the stimulus as our feature matrix. Thus, our number of features will be equal to the number of simultaneously recorded neurons.\n",
    "\n",
    "It's important to note that the choice of which feature to use is a design choice on the part of the scientist. Instead of using the average response, we could make many other choices such as using the max response, the instantaneous response at some point in time, or even maintaining the full dynamic timecourse (though this latter choice would dramatically increase our dimensionality)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## first some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os is a library of standard operating system functions\n",
    "import os\n",
    "\n",
    "# numpy is a library of mathematical functions for manipulating arrays\n",
    "import numpy as np\n",
    "\n",
    "# pandas is a library of data analysis tools\n",
    "import pandas as pd\n",
    "\n",
    "# matplotlib is a plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# seaborn is a library of plotting functions built on top of matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "# The allensdk is the institute toolset of interacting with data\n",
    "from allensdk.brain_observatory.behavior.behavior_project_cache import VisualBehaviorOphysProjectCache\n",
    "\n",
    "# mindscope_utilities contains convenience functions built on top of the AllenSDK\n",
    "import mindscope_utilities\n",
    "import mindscope_utilities.visual_behavior_ophys as ophys\n",
    "\n",
    "# This sets the number of columns that will be displayed by default when viewing Pandas tables\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "platstring = platform.platform()\n",
    "\n",
    "if ('amzn1' in platstring):\n",
    "    # for AWS\n",
    "    data_storage_directory = \"/data/visual-behavior-ophys-data\"\n",
    "    # use local cache for AWS\n",
    "    cache = VisualBehaviorOphysProjectCache.from_local_cache(cache_dir=data_storage_directory, use_static_cache=True)\n",
    "else:  \n",
    "    # for local drive, different operating systems\n",
    "    if 'Darwin' in platstring:\n",
    "        # OS X \n",
    "        data_root = \"/Volumes/Brain2021/\"\n",
    "    elif 'Windows'  in platstring:\n",
    "        # Windows (replace with the drive letter of USB drive)\n",
    "        data_root = \"E:/\"\n",
    "        #data_root = r'//allen/programs/braintv/workgroups/nc-ophys/visual_behavior/platform_paper_cache'\n",
    "    else:\n",
    "        # your own linux platform\n",
    "        # EDIT location where you mounted hard drive\n",
    "        data_root = \"/media/$USERNAME/Brain2021/\"\n",
    "        data_root = \"/run/media/tom.chartrand/Brain2021\"\n",
    "        \n",
    "    # visual behavior cache directory\n",
    "    cache_dir = manifest_path = os.path.join(data_root, \"visual_behavior\")\n",
    "    # use from_s3_cache for loading from local directory\n",
    "    cache = VisualBehaviorOphysProjectCache.from_s3_cache(cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next we need to get some data loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up data paths, load session/experiment table\n",
    "Recall from the Visual Behavior data overview that we have hundreds of behavior/ophys sessions, each with up to 8 simultaneously acquired imaging planes (aka 'experiments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = VisualBehaviorOphysProjectCache.from_s3_cache(cache_dir=data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_table = cache.get_ophys_session_table()\n",
    "experiment_table = cache.get_ophys_experiment_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load one example session\n",
    "We are going to select one session from the session table, session 990139534. This is a session with Sst-IRES-Cre mouse, which expressed GCaMP6f in somatostatin positive inhibitory interneurons. There were 6 simultaneously acquired imaging planes for this session. \n",
    "We can view metadata for this session as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ophys_session_id = 854060305\n",
    "session_table.loc[ophys_session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the difference in selection:\n",
    "print(type(session_table.loc[ophys_session_id])) # single brackets \n",
    "print(type(session_table.loc[[ophys_session_id]])) # double brackets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download all associated experiments\n",
    "\n",
    "Each session consists of one or more 'experiments', in which each experiment is a single imaging plane\n",
    "\n",
    "Each mesoscope session has up to 8 experiments (this session has 6) associated with the session. We will load all sessions into a dictionary with the experiment IDs as the keys\n",
    "\n",
    "The first time that this cell is run, the associated NWB files will be downloaded to your local `data_storage_directory`. Subsequent runs of this cell will be faster since the data will already be cached locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = {}\n",
    "ophys_experiment_ids = session_table.loc[ophys_session_id]['ophys_experiment_id']\n",
    "for ophys_experiment_id in ophys_experiment_ids:\n",
    "    experiments[ophys_experiment_id] = cache.get_behavior_ophys_experiment(ophys_experiment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restructure neural data\n",
    "\n",
    "The cell below will load the neural data into a pandas 'tidy' format by iterating over each of the 6 experiments and using some helpful tools from the `visual_behavior_ophys` module of the `mindscope_utilities` package that was imported above as `ophys`. \n",
    "\n",
    "It will also include a subset of metadata from `ophys_experiment_table` to facilitate splitting by depth, structure (aka cortical area), cre line (aka cell class), etc.\n",
    "\n",
    "Note that 'tidy' data means that each row represents only one observation. Observations are stacked vertically. Thus, the `timestamps` colums will repeat for every cell in the dataset.\n",
    "\n",
    "Also note that this could fail if you are on a machine or cloud instance with too little available RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also use tqdm package to display progress bar.\n",
    "# EXAMPLE:\n",
    "# _________\n",
    "# from tqdm import tqdm\n",
    "# for i in tqdm(I):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty list\n",
    "neural_data = []\n",
    "for ii, ophys_experiment_id in enumerate(experiments.keys()): \n",
    "    print('on experiment {} of {}'.format(ii+1, len(experiments)), end='\\r')\n",
    "    this_experiment = experiments[ophys_experiment_id]\n",
    "    \n",
    "    # append the data for this experiment to a list\n",
    "    neural_data.append(ophys.build_tidy_cell_df(this_experiment))\n",
    "    \n",
    "# concatate the list of dataframes into a single dataframe\n",
    "neural_data = pd.concat(neural_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(neural_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe is over 2 million rows long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(neural_data['cell_specimen_id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it contains the timeseries of 53 unique simultaneously recorded neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up data for a classification algorithm and scikit-learn\n",
    "We need to get the data into a standard format for the classification algorithms in scikit learn, which is often a feature matrix (**`X`**) and a vector of labels (**`y`**).\n",
    "\n",
    "For this analysis, let's look at the responses to each of the **8 unique visual stimuli** in this session, plus the response to **omitted** stimuli.\n",
    "\n",
    "### make some decisions about the format of `X`\n",
    "Our goal is to construct a feature matrix that consists of the the average response of each cell in a short (750ms) window following each stimulus on every stimulus presentation. Each stimulus_presentation will be represented by an `n_neurons` dimension feature vector. Our feature matrix, `X`, will thus be `n_trials x n_neurons`\n",
    "\n",
    "But as noted in the introduction, we could have made many other choices about the format of `X`. An interesting exercise would be to repeat the analyses that follow with different choices for the feature matrix `X`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the full response to each stimulus\n",
    "First, we will calculate an event triggered response to each stimulus start time in the stimulus table.\n",
    "\n",
    "To start with, let's define a simple wrapper function on the `mindscope_utilities.event_triggered_response` function. We are going to use the deltaF/F response in a window from 0 to 750 ms from every stimulus event to calculate the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_event_triggered_response(single_cell_data, event_times):\n",
    "    event_triggered_response = mindscope_utilities.event_triggered_response(\n",
    "        single_cell_data,\n",
    "        t = 'timestamps',\n",
    "        y = 'dff',\n",
    "        event_times = event_times,\n",
    "        t_before = 0,\n",
    "        t_after = 0.75,\n",
    "        output_sampling_rate = 30 \n",
    "    )\n",
    "    \n",
    "    return event_triggered_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to define our 'event_times' as the start time of *every* unique stimulus. First we will load the stimulus table from one of our experiments. Recall that each experiment in a given session shares the same stimulus data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stimulus_presentations = experiments[ophys_experiment_ids[0]].stimulus_presentations.drop(columns = ['image_set']) # dropping the 'image_set' column to avoid confusion. Image_set column contains a unique string for set of images presented in a session.\n",
    "stimulus_presentations.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will select our event times as the `start_time` of each stimulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_times = stimulus_presentations['start_time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now iterate over every cell and apply the above function to build out an event triggered response for every cell/stimulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate an empty list. We will append every event triggered response to this list\n",
    "full_etr = []\n",
    "\n",
    "# iterate over each unique cell\n",
    "cell_specimen_ids = neural_data['cell_specimen_id'].unique()\n",
    "\n",
    "for cell_count, cell_specimen_id in enumerate(cell_specimen_ids):\n",
    "    print('on cell {}, #{} of {}'.format(cell_specimen_id, cell_count+1, len(cell_specimen_ids)), end='\\r')\n",
    "    \n",
    "    # calculate the event triggered response for this cell\n",
    "    full_etr_this_cell = calculate_event_triggered_response(\n",
    "        neural_data.query('cell_specimen_id == @cell_specimen_id'),\n",
    "        event_times\n",
    "    )\n",
    "    \n",
    "    # add a column identifying the cell_specimen_id\n",
    "    full_etr_this_cell['cell_specimen_id'] = cell_specimen_id\n",
    "    # append to our list\n",
    "    full_etr.append(full_etr_this_cell)\n",
    "    \n",
    "# concatenate our list of dataframes into a single dataframe\n",
    "full_etr = pd.concat(full_etr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting dataframe includes the full response of every cell to every stimulus event on a common 30 Hz timebase. We can view it as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_etr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But as noted above, what we're interested in is just the *average* response of each cell to each stimulus. We can use the Pandas `groupby` command to group by `cell_specimen_id` and `event_number`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_responses = full_etr.groupby(\n",
    "    ['cell_specimen_id','event_number']\n",
    ")[['dff']].mean().reset_index()\n",
    "average_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `event_triggered_response` function returns a column called `event_number`. Because we passed in every event in the stimulus table, this number will correspond directly with the `stimulus_presentations_id` column in the stimulus table (see above). We will rename it for clarity and to facilitate merges below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_responses.rename(columns={'event_number':'stimulus_presentations_id'}, inplace=True)\n",
    "average_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can easily merge in stimulus information from the stimulus table by merging on the `stimulus_presentations_id` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_responses = average_responses.merge(\n",
    "    stimulus_presentations,\n",
    "    on = 'stimulus_presentations_id',\n",
    "    how = 'left'\n",
    ")\n",
    "average_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can construct a dataframe called `features_and_labels` that will contain one row per trial, one column per cell, plus columns with the image_index and image_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_and_labels = average_responses.pivot(\n",
    "    index = 'stimulus_presentations_id',\n",
    "    columns = 'cell_specimen_id',\n",
    "    values = 'dff'\n",
    ").merge(\n",
    "    stimulus_presentations[['image_index','image_name']],\n",
    "    on = 'stimulus_presentations_id',\n",
    "    how = 'left'\n",
    ")\n",
    "features_and_labels.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `X` matrix can be extracted by getting the columns associated with the cell_specimen_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features_and_labels[cell_specimen_ids]\n",
    "X.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And `y` is just the image_name column (it could also be the image_index column if you want a numeric value instead of a string to represent the image identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = features_and_labels['image_name']\n",
    "y.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction\n",
    "One way of visualizing the data is to use a dimensionality reduction technique. For example, we can use t-SNE, which will project our n-neuron-dimensional feature space into two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X_embedded = TSNE(n_components=2).fit_transform(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " X_embedded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And visualize the results, with colors representing each unique stimulus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_and_labels['tsne-2d-one'] = X_embedded[:,0]\n",
    "features_and_labels['tsne-2d-two'] = X_embedded[:,1]\n",
    "plt.figure(figsize=(16,10))\n",
    "ax = sns.scatterplot(\n",
    "    data=features_and_labels,\n",
    "    x=\"tsne-2d-one\", \n",
    "    y=\"tsne-2d-two\",\n",
    "    hue=\"image_name\",\n",
    "    hue_order = np.sort(features_and_labels['image_name'].unique()),\n",
    "    palette='tab10',\n",
    "    legend=\"full\",\n",
    "    alpha=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demonstrates that the time-averaged population responses to at least some of the stimuli seem to fall into distinct clusters in our higher dimensional space, while others appear more overlapped. This implies that a decoding analysis might be more successful at decoding some stimuli than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Linear Discriminant Analysis decoder\n",
    "We can use an Linear Discriminant Analysis decoder from scikit learn to ask how well we can decode image identity from the feature matrix we have constructed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split our data into train and test sets, instantiate the model, then fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "model = LDA(store_covariance=True)\n",
    "model.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the model to make predictions on the held-out test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can construct a confusion matrix as follows. Note that we are converting the matrix to a dataframe just to make axis labeling easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_df = pd.DataFrame(\n",
    "    confusion_matrix(y_test, y_pred, normalize = True), \n",
    "    columns = ['predicted_{}'.format(im) for im in model.classes_],\n",
    "    index = ['actual_{}'.format(im) for im in model.classes_]\n",
    ")\n",
    "confusion_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if above normalization step doesnt work:\n",
    "# confusion_df = confusion_df/confusion_df.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can also visualize our confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "im = ax.imshow(confusion_df, aspect='auto')\n",
    "\n",
    "ax.set_xticks(np.arange(0,9))\n",
    "ax.set_xticklabels(model.classes_)\n",
    "ax.set_xlabel('predicted stimulus')\n",
    "\n",
    "ax.set_yticks(np.arange(0,9))\n",
    "ax.set_yticklabels(model.classes_)\n",
    "ax.set_ylabel('actual stimulus')\n",
    "\n",
    "plt.colorbar(im, label='fraction correct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we can plot the feature matrix after sorting by image ID\n",
    "This helps explain how the classifier might be working: some cells are very image selective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15,5))\n",
    "\n",
    "sorted_matrix = features_and_labels.sort_values(by='image_index')\n",
    "ax.imshow(sorted_matrix[cell_specimen_ids], aspect='auto', clim=[0,0.1], interpolation='none')\n",
    "\n",
    "# add a white line to demarcate every distinct image\n",
    "demarcations = []\n",
    "sorted_matrix['last_image_index'] = sorted_matrix['image_index'].shift()\n",
    "for idx, row in sorted_matrix.reset_index().iterrows():\n",
    "    if row['last_image_index'] != row['image_index']:\n",
    "        ax.axhline(idx, color='white')\n",
    "        demarcations.append(idx)\n",
    "demarcations.append(idx)\n",
    "\n",
    "# set yticks at the halfway points between demarcations\n",
    "ax.set_yticks([(demarcations[i] + demarcations[i+1])/2 for i in range(len(demarcations)-1)])      \n",
    "# assign yticklabels as image_name\n",
    "ax.set_yticklabels(sorted_matrix['image_name'].unique())\n",
    "\n",
    "ax.set_ylabel('trials (sorted by image_name)')\n",
    "ax.set_xlabel('neuron')\n",
    "\n",
    "for i in range(0, len(cell_specimen_ids), 1):\n",
    "    ax.axvline(i-0.5, color='white')\n",
    "\n",
    "\n",
    "ax.set_title('average responses sorted by image index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And here are the images that the mouse saw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = experiments[ophys_experiment_ids[0]]\n",
    "fig, ax = plt.subplots(2,4,figsize = (20,8), sharex = True, sharey=True)\n",
    "for ii,image_name in enumerate(experiment.stimulus_templates.index):\n",
    "    ax.flatten()[ii].imshow(experiment.stimulus_templates.loc[image_name]['unwarped'], cmap='gray')\n",
    "    ax.flatten()[ii].set_title(image_name)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuromatch",
   "language": "python",
   "name": "neuromatch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
